---
title: "Exponential Smoothing"
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, collapse = TRUE)
```

Exponential forecasting is another smoothing method and has been around since the 1950s.  Where [niave forecasting](ts_benchmarking#naive) places 100% weight on the most recent observation and [moving averages](ts_moving_averages) place equal weight on *k* values, exponential smoothing allows for weighted averages where greater weight can be placed on recent observations and lesser weight on older observations. Exponential smoothing methods are intuitive, computationally efficient, and generally applicable to a wide range of time series. Consequently, exponentially smoothing is a great forecasting tool to have and this tutorial will walk you through the basics.

<br>

## tl;dr

1. [Replication Requirements](#replication): What you'll need to reproduce the analysis in this tutorial.
2. [Simple Exponential Smoothing](#ses): Technique for data with no trend or seasonality.
3. [Holt's Method](#holts): Technique for data with trend but no seasonality.
4. [Holt-Winters Seasonal Method](#hw): Technique for data with trend *and* seasonality.
5. [Damped Trend Methods](#damp): Technique for trends that are believed to become more conservative or "flat-line" over time.
6. [Exercises](#exercises): Practice what you've learned


<br>

## Replication Requirements {#replication}

This tutorial primarily uses the `fpp2` package. `fpp2` will automatically load the `forecast` package (among others), which provides many of the key forecasting functions used throughout.  

```{r message=FALSE}
library(tidyverse)
library(fpp2)          
```

Furthermore, we'll use a couple data sets to illustrate.  The `goog` and `qcement` data are provided by the `fpp2` package. Let's go ahead and set up training and validation sets:

```{r}
# create training and validation of the Google stock data
goog.train <- window(goog, end = 900)
goog.test <- window(goog, start = 901)

# create training and validation of the AirPassengers data
qcement.train <- window(qcement, end = c(2012, 4))
qcement.test <- window(qcement, start = c(2013, 1))
```


<br>

## Simple Exponential Smoothing {#ses}

The simplest of the exponentially smoothing methods is called "simple exponential smoothing" (SES).  The key point to remember is that SES is **suitable for data with no trend or seasonal pattern**. This section will illustrate why.

For exponential smoothing, we weigh the recent observations more heavily than older observations. The weight of each observation is determined through the use of a *smoothing parameter*, which we will denote $\alpha$.  For a data set with $T$ observations, we calculate our predicted value, $\hat{y}_{T+1}$, which will be based on $y_{1}$ through $y_{T}$ as follows:

$$
\hat{y}_{T+1} = \alpha{y_T} + \alpha(1-\alpha)y_{T-1} + \dots + \alpha(1-\alpha)^{T-1}y_{1}
$$

where $0 < \alpha \leq 1$. It is also common to come to use the *component form* of this model, which uses the following set of equations.

$$
\hat{y}_{t+1} = l_{t}
$$

$$
l_{t} = \alpha{y_{t}} + (1 - \alpha)l_{t-1}
$$

In both equations we can see that the most weight is placed on the most recent observation. In practice, $\alpha$ equal to 0.1-0.2 tends to perform quite well but we'll demonstrate shortly how to tune this parameter.  When $\alpha$ is closer to 0 we consider this *slow learning* because the algorithm gives historical data more weight.  When $\alpha$ is closer to 1 we consider this *fast learning* because the algorithm gives more weight to the most recent observation; therefore, recent changes in the data will have a bigger impact on forecasted values. The following table illustrates how weighting changes based on the $\alpha$ parameter:

<div id="alpha-chart" class="section level1" style="width: 100%;">
<table style="font-size:13px;">
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
<thead>
<tr class="header">
<th align="left">Observation</th>
<th align="left">$\alpha=0.2$</th>
<th align="left">$\alpha=0.4$</th>
<th align="left">$\alpha=0.6$</th>
<th align="left">$\alpha=0.8$</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th align="left">$y_T$</th>
<th align="left">0.2</th>
<th align="left">0.4</th>
<th align="left">0.6</th>
<th align="left">0.8</th>
</tr>
<tr class="even">
<th align="left">$y_{T-1}$</th>
<th align="left">0.16</th>
<th align="left">0.24</th>
<th align="left">0.26</th>
<th align="left">0.16</th>
</tr>
<tr class="odd">
<th align="left">$y_{T-2}$</th>
<th align="left">0.128</th>
<th align="left">0.144</th>
<th align="left">0.096</th>
<th align="left">0.032</th>
</tr>
<tr class="even">
<th align="left">$y_{T-3}$</th>
<th align="left">0.1024</th>
<th align="left">0.0864</th>
<th align="left">0.0384</th>
<th align="left">0.0064</th>
</tr>
<tr class="odd">
<th align="left">$\vdots$</th>
<th align="left">$\vdots$</th>
<th align="left">$\vdots$</th>
<th align="left">$\vdots$</th>
<th align="left">$\vdots$</th>
</tr>
</tbody>
</table>
</div>

Let's go ahead and apply SES to the Google data using the `ses` function.  We manually set the $\alpha = .2$ for our initial model and forecast forward 100 steps with $h=100$. We see that our forecast projects a flatlined estimate into the future, which does not capture the positive trend in the data. This is why SES should not be used on data with a trend or seasonal component.

```{r es1, fig.align='center', fig.height=4, fig.width=8}
ses.goog <- ses(goog.train, alpha = .2, h = 100)
autoplot(ses.goog)
```

One approach to correct for this is to difference our data to remove the trend.  Now, `goog.dif` represents the change in stock price from the previous day.

```{r es2, fig.align='center', fig.height=4, fig.width=8}
goog.dif <- diff(goog.train)
autoplot(goog.dif)
```

Once we've differenced we've effectively removed the trend from our data and can reapply the SES model.

```{r es3, fig.align='center', fig.height=4, fig.width=8}
ses.goog.dif <- ses(goog.dif, alpha = .2, h = 100)
autoplot(ses.goog.dif)
```

To understand how well the model predicts we can compare our forecasts to our validation data set.  But first we need to create a differenced validation set since our training data was built on differenced data.  We see that performance measures are smaller on the test set than the training so we are not overfitting our model.

```{r}
goog.dif.test <- diff(goog.test)
accuracy(ses.goog.dif, goog.dif.test)
```

In our model we used the standard $\alpha = 0.20$; however, we can tune our alpha parameter to identify the value that reduces our forecasting error.  Here we loop through alpha values from 0.01-0.99 and identify the level that minimizes our test RMSE.  Turns out that $\alpha = 0.05$ minimizes our prediction error.

```{r es4, fig.align='center', fig.height=4, fig.width=6}
# identify optimal alpha parameter
alpha <- seq(.01, .99, by = .01)
RMSE <- NA
for(i in seq_along(alpha)) {
  fit <- ses(goog.dif, alpha = alpha[i], h = 100)
  RMSE[i] <- accuracy(fit, goog.dif.test)[2,2]
}

# convert to a data frame and idenitify min alpha value
alpha.fit <- data_frame(alpha, RMSE)
alpha.min <- filter(alpha.fit, RMSE == min(RMSE))

# plot RMSE vs. alpha
ggplot(alpha.fit, aes(alpha, RMSE)) +
  geom_line() +
  geom_point(data = alpha.min, aes(alpha, RMSE), size = 2, color = "blue")  
```

Now we can re-fit out SES with $\alpha = 0.05$.  Our performance metrics are not significantly different from our model where $\alpha = 0.20$; however, you will notice that the predicted confidence intervals are narrower (left chart).  And when we zoom into the predicted versus actuals (right chart) you see that for most observations, our predicted confidence intervals did well.

```{r es5, fig.align='center', fig.height=4, fig.width=10}
# refit model with alpha = .05
ses.goog.opt <- ses(goog.dif, alpha = .05, h = 100)

# performance eval
accuracy(ses.goog.opt, goog.dif.test)

# plotting results
p1 <- autoplot(ses.goog.opt) +
  theme(legend.position = "bottom")
p2 <- autoplot(goog.dif.test) +
  autolayer(ses.goog.opt, alpha = .5) +
  ggtitle("Predicted vs. actuals for the test data set")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

<br>

## Holt's Method {#holts}

As mentioned and observed in the previous section, SES does not perform well with data that has a long-term trend. In the last section we illustrated how you can remove the trend with differencing and then perform SES.  An alternative method to apply exponential smoothing while capturing trend in the data is to use _Holt's Method_.  

Holt's Method makes predictions for data with a trend using _**two**_ smoothing parameters, $\alpha$ and $\beta$, which correspond to the level and trend components, respectively. For Holt's method, the prediction will be a line of some non-zero slope that extends from the time step after the last collected data point onwards.

The methodology for predictions using data with a trend (Holt's Method) uses the following equation with $T$ observations. The *k*-step-ahead forecast is given by combining the level estimate at time *t* ($L_t$) and the trend estimate (which in this example is assumbed additive) at time *t* ($T_t$). 

$$
\hat{y}_{T+1} = L_t + kT_t
$$

The level ($L_t$) and trend ($T_t$) are updated through a pair of updating equations, which is where you see the presence of the two smoothing paramters:

$$
L_t = \alpha{y_t} + \alpha(1-\alpha) (L_{t-1} + T_{t-1}),
$$

$$
T_t = \beta(L_t - L_{t-1}) + (1-\beta)T_{t-1}.
$$

In these equations, the first means that the level at time *t* is a weighted average of the actual value at time *t* and the level in the previous period, adjusted for trend.  The second equation means that the trend at time *t* is a weighted average of the trend in the previous period and the more recent information on the change in the level. Similar to SES, $\alpha$ and $\beta$ are constrained to 0-1 with higher values giving faster learning and lower values providing slower learning.

To capture a _**multiplicative**_ (exponential) trend we make a minor adjustment in the above equations:

$$
\hat{y}_{T+1} = L_t \times kT_t
$$

$$
L_t = \alpha{y_t} + \alpha (1 - \alpha) (L_{t-1} \times {T_t-1}),
$$

$$
T_t = \beta(L_t \div L_{t-1}) + (1-\beta)T_{t-1}.
$$

Holt's method also has the alternative _Component Form_ operations.  In this case these represent the additive trend component form:

$$
\hat{y}_{t+h} = l_{t} + hb_{t}
$$

$$
l_{t} = {\alpha}y_{t} + (1 - \alpha)(l_{t-1} + b_{t-1})
$$
$$
b_{t} = {\beta}(l_{t} - l_{t-1}) + (1 - \beta)b_{t-1}
$$

If we go back to our Google stock data, we can apply Holt's method in the following manner.  Here, we will not manually set the $\alpha$ and $\beta$ for our initial model and forecast forward 100 steps with $h=100$. We see that our forecast now does a better job capturing the positive trend in the data.

```{r es6, fig.align='center', fig.height=4, fig.width=8}
holt.goog <- holt(goog.train, h = 100)
autoplot(holt.goog)
```

Within `holt` you can manually set the $\alpha$ and $\beta$ parameters; however, if you leave those parameters as NULL, the `holt` function will actually identify the optimal model parameters. It does this by minimizing AIC and BIC values.  We can see the model selected by `holt`. In this case, $\alpha = 0.9967$ meaning fast learning in the day-to-day movements and $\beta = 0.0001$ which means slow learning for the trend. 

```{r}
holt.goog$model
```

Let's check the predictive accuracy of our model.  According to our MAPE we have about a 2% error rate.

```{r}
accuracy(holt.goog, goog.test)
```

Similar to SES, we can tune the $\beta$ parameter to see if we can improve our predictive accuracy.  The `holt` function identified an optimal $\beta = 0.0001$; however, this optimal value is based on minimizing errors on the training set, not minimizing prediction errors on the test set.  Let's assess a tradespace of $\beta$ values and see if we gain some predictive accuracy.  Here, we loop through a series of $\beta$ values starting at 0.0001 all the way up to 0.5.  We see that there is a dip in our RMSE at 0.0601.

```{r es7, fig.align='center', fig.height=4, fig.width=6}
# identify optimal alpha parameter
beta <- seq(.0001, .5, by = .001)
RMSE <- NA
for(i in seq_along(beta)) {
  fit <- holt(goog.train, beta = beta[i], h = 100)
  RMSE[i] <- accuracy(fit, goog.test)[2,2]
}

# convert to a data frame and idenitify min alpha value
beta.fit <- data_frame(beta, RMSE)
beta.min <- filter(beta.fit, RMSE == min(RMSE))

# plot RMSE vs. alpha
ggplot(beta.fit, aes(beta, RMSE)) +
  geom_line() +
  geom_point(data = beta.min, aes(beta, RMSE), size = 2, color = "blue")  
```

Now let's refit our model with this optimal $\beta$ value and compare our predictive accuracy to our original model.  We see that our new model reduces our error rate (MAPE) down to 1.78%.  

```{r}
# new model with optimal beta
holt.goog.opt <- holt(goog.train, h = 100, beta = 0.0601)

# accuracy of first model
accuracy(holt.goog, goog.test)

# accuracy of new optimal model
accuracy(holt.goog.opt, goog.test)
```

If we plot our original versus more recent optimal model we'll notice a couple things.  First, our predicted values for the optimal model are more conservative; in other words they are assuming a more gradual slope. Second, the confidence intervals are much more extreme.  So although our predictions were more accurate, our uncertainty increases.  The reason for this is that by increasing our $\beta$ value we are assuming faster learning from more recent observations.  And since there some quite a bit of turbulence in the recent time period, this is causing greater variance to be incorporated into our prediction intervals.  This requires a more indepth discussion than this tutorial will go into, but the important thing to keep in mind is that although we increase our prediction accuracy with parameter tuning, there are additional side effects that can occur, which may be harder to explain to decision-makers.

```{r es8, fig.align='center', fig.height=4, fig.width=10}
p1 <- autoplot(holt.goog) +
  ggtitle("Original Holt's Model") +
  coord_cartesian(ylim = c(400, 1000))

p2 <- autoplot(holt.goog.opt) +
  ggtitle("Optimal Holt's Model") +
  coord_cartesian(ylim = c(400, 1000))

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

<br>

## Holt-Winters Seasonal Method {#hw}

To make predictions using data with a trend and seasonality, we turn to the Holt-Winters Seasonal Method. This method can be implemented with an "Additive" structure or a "Multiplicative" structure, where the choice of method depends on the data set. The Additive model is best used when the seasonal trend is of the same magnitude throughout the data set, while the Multiplicative Model is preferred when the magnitude of seasonality changes as time increases.

Since the Google data does not have seasonality, we'll use the `qcement` data that we set up in the [Replication section](#replication) to demonstrate.  This data has seasonality and trend; however, it is unclear if seasonality is additive or multiplicative.  We'll use the Holt-Winters method to identify the best fit model.

```{r es9, fig.align='center', fig.height=5, fig.width=8}
autoplot(decompose(qcement))
```


### Additive
For the Additive model, the regular equation form is:

$$
\hat{y}_{T+1} = L_t + kT_t + S_{t+k-m}
$$

The level ($L_t$), trend ($T_t$), and season ($S_t$) are updated through a pair of updating equations, which is where you see the presence of the three smoothing paramters:

$$
L_t = \alpha(y_t - S_{t-m}) + (1-\alpha)(L_{t-1} + T_{t-1}),
$$

$$
T_t = \beta(L_t - L_{t-1}) + (1-\beta)T_{t-1},
$$

$$
S_t = \gamma(y_t - L_t) + (1-\gamma)S_{t-m}.
$$

where $\alpha$, $\beta$, and $\gamma$ are the three smoothing parameters to deal with the level pattern, the trend, and the seasonality, respectively. Similar to SES and Holt's method, all three parameters are constrained to 0-1.  The component equations are as follows:

$$
\hat{y}_{t+h} = l_{t} + hb_{t} + s_{t-m+h^{+}_{m}}
$$

$$
l_{t} = \alpha(y_{t} - s_{t-m}) + (1 - \alpha)(l_{t-1} + b_{t-1})
$$

$$
b_{t} = \beta(l_{t} - l_{t-1}) + (1 - \beta)b_{t-1}
$$

$$
s_{t} = \gamma(y_{t} - l_{t-1} - b_{t-1}) + (1-\gamma)s_{t-m}
$$

To apply the Holt-Winters method we'll introduce a new function, `ets` which stands for error, trend, and seasonality.  The important thing to understand about the `ets` model is how to select the `model =` parameter.  In total you have 36 model options to choose from.  The parameter settings in the below code (`model = "AAA"`) stands for a model with additive error, additive trend, and additve seasonality. 

```{r es10, fig.align='center', fig.height=4, fig.width=6}
qcement.hw <- ets(qcement.train, model = "AAA")
autoplot(forecast(qcement.hw))
```


 So when specificying the model type you always specificy the error, trend, then seasonality (hence "ets").  The options you can specify for each component is as follows:
 
 - error: additive ("A"), multiplicative ("M"), unknown ("Z")
 - trend: none ("N"), additive ("A"), multiplicative ("M"), unknown ("Z")
 - seasonality: none ("N"), additive ("A"), multiplicative ("M"), unknown ("Z")

Consequently, if you wanted to apply a Holt's model where the error and trend were additive and no seasonality exists you would select `model = "AAN"`.  If you want to apply a Holt-Winters model where there is additive error, an exponential (multiplicative) trend, and additive seasonality you would select `model = "AMA"`.  If you are uncertain of the type of component then you use "Z".  So if you were uncertain of the components or if you want the model to select the best option, you could use `model = "ZZZ"` and the "optimal" model will be selected.

If we assess our additive model we can see that $\alpha = 0.6208$, $\beta = 0.0001$, and $\gamma = 0.1913$

```{r}
summary(qcement.hw)
```

If we check our residuals, we see that residuals grow larger over time.  This may suggest that a multiplicative error rate may be more appropriate.

```{r es11, fig.align='center', fig.height=5, fig.width=6}
checkresiduals(qcement.hw)
```


If we check the predictive accuracy we see that our prediction accuracy is about 2.9% (according to the MAPE).  

```{r}
# forecast the next 5 quarters
qcement.f1 <- forecast(qcement.hw, h = 5)

# check accuracy
accuracy(qcement.f1, qcement.test)
```

### Multiplicative

As previously stated, we may have multiplicative features for our Holt-Winters method.  If we have multiplicative seasonality then our equation form changes to:

$$
\hat{y}_{T+1} = (L_t + kT_t)S_{t+k-m}
$$

The level ($L_t$), trend ($T_t$), and season ($S_t$) are updated through a pair of updating equations, which is where you see the presence of the three smoothing paramters:

$$
L_t = \alpha y_t / S_{t-m} + (1-\alpha)(L_{t-1} + T_{t-1}),,
$$

$$
T_t = \beta(L_t - L_{t-1}) + (1-\beta)T_{t-1},
$$

$$
S_t = \gamma(y_t / L_t) + (1-\gamma)S_{t-m}.
$$

If we apply a multiplicative seasonality model then our `model` parameter becomes `model = "MAM"` (here, we are actually applying a multiplicative error and seasonality model). We see that are residuals illustrate less change in magnitude over time.  We still have an issue with autocorrelation with errors but we'll address that in later tutorials.

```{r es12, fig.align='center', fig.height=5, fig.width=6}
qcement.hw2 <- ets(qcement.train, model = "MAM")
checkresiduals(qcement.hw2)
```

To compare the predictive accuracy of our models let's compare four different models.  We see that the first model (additive error, trend and seasonality) results in the lowest RMSE and MAPE on our test data set.

```{r}
# additive error, trend and seasonality
qcement.hw1 <- ets(qcement.train, model = "AAA")
qcement.f1 <- forecast(qcement.hw1, h = 5)
accuracy(qcement.f1, qcement.test)

# multiplicative error, additive trend and seasonality
qcement.hw2 <- ets(qcement.train, model = "MAA")
qcement.f2 <- forecast(qcement.hw2, h = 5)
accuracy(qcement.f2, qcement.test)

# additive error and trend and multiplicative seasonality
qcement.hw3 <- ets(qcement.train, model = "AAM", restrict = FALSE)
qcement.f3 <- forecast(qcement.hw3, h = 5)
accuracy(qcement.f3, qcement.test)

# multiplicative error, additive trend, and multiplicative seasonality
qcement.hw4 <- ets(qcement.train, model = "MAM")
qcement.f4 <- forecast(qcement.hw4, h = 5)
accuracy(qcement.f4, qcement.test)
```

If we were to compare this to an unspecified model where we let `ets` select the optimal model, we see that `ets` selects a model specification of multiplicative error, additive trend, and multiplicative seasonality ("MAM").  This is equivalent to our fourth model above.  This model is assumed "optimal" because it minimizes RMSE, AIC, and BIC on the _**training**_ data set, but does not necessarily minimize prediction errors on the test set.  

```{r}
qcement.hw5 <- ets(qcement.train, model = "ZZZ")
summary(qcement.hw5)
```

As we did in the [SES](#ses) and [Holt's method](#holts) section, we can optimize the $\gamma$ parameter in our Holt-Winters model.  Here, we use the additive error, trend and seasonality model that minimized our prediction errors above and identify the $\gamma$ parameter that minimizes forecast errors. In this case we see that $\gamma = 0.21$ minimizes the error rate.

```{r es13, fig.align='center', fig.height=4, fig.width=6}
gamma <- seq(0.01, 0.85, 0.01)
RMSE <- NA

for(i in seq_along(gamma)) {
  hw.expo <- ets(qcement.train, "AAA", gamma = gamma[i])
  future <- forecast(hw.expo, h = 5)
  RMSE[i] = accuracy(future, qcement.test)[2,2]
}

error <- data_frame(gamma, RMSE)
minimum <- filter(error, RMSE == min(RMSE))
ggplot(error, aes(gamma, RMSE)) +
  geom_line() +
  geom_point(data = minimum, color = "blue", size = 2) +
  ggtitle("gamma's impact on forecast errors",
          subtitle = "gamma = 0.21 minimizes RMSE")
```

If we update our model with this "optimal" $\gamma$ parameter we see that we bring our forecasting error rate down from 2.88% to 2.76%.  This is a small improvement, but often small improvements can have large business implications.

```{r}
# previous model with additive error, trend and seasonality
accuracy(qcement.f1, qcement.test)

# new model with optimal gamma parameter
qcement.hw6 <- ets(qcement.train, model = "AAA", gamma = 0.21)
qcement.f6 <- forecast(qcement.hw6, h = 5)
accuracy(qcement.f6, qcement.test)
```

With this new optimal model we can get our predicted values:

```{r}
qcement.f6
```

and also visualize these predicted values:

```{r es14, fig.align='center', fig.height=4, fig.width=6}
autoplot(qcement.f6)
```

<br>

## Damping Methods {#damp}

One last item to discuss is the idea of "damping" your forecast.  Damped forecasts use a damping coefficient denoted $\phi$ to more conservatively estimate the predicted trend.  Basically, if you believe that your additive or multiplicative trend is or will be slowing down ("flat-lining") in the near future then you are assuming it will dampen.

The equation form for an additive model with a damping coefficient is

$$
\hat{y}_{T+h} = L_t + (\phi + \phi^2 + \cdots + \phi^h)\beta_t
$$

$$
L_t = \alpha{y_t} + \alpha(1-\alpha) (L_{t-1} + \phi \beta_{t-1}),
$$

$$
\beta_t = \beta(L_t - L_{t-1}) + (1-\beta)\phi \beta_{t-1}.
$$

where $0 < \phi < 1$.  When $\phi = 1$ the method is the same as Holt's additive model.  As $\phi$ gets closer to 0, the trend becomes more conservative and flat-lines to a constant in the nearer future.  The end result of this method is that short-run forecasts are still trended while long-run forecasts are constant.

To illustrate the effect of a damped forecast we'll use the `fpp2::ausair` data set.  Here, we create several models (additive, additive + damped, multiplicative, multiplicative + damped).  In the plot you can see that the damped models (dashed lines) have more conservative trend lines and if we forecasted these far enough into the future we would see this trend flat-line.

```{r es15, fig.align='center', fig.height=4, fig.width=6}
# holt's linear (additive) model
fit1 <- ets(ausair, model = "ZAN", alpha = 0.8, beta = 0.2)
pred1 <- forecast(fit1, h = 5)

# holt's linear (additive) model
fit2 <- ets(ausair, model = "ZAN", damped = TRUE, alpha = 0.8, beta = 0.2, phi = 0.85)
pred2 <- forecast(fit2, h = 5)

# holt's exponential (multiplicative) model
fit3 <- ets(ausair, model = "ZMN", alpha = 0.8, beta = 0.2)
pred3 <- forecast(fit3, h = 5)

# holt's exponential (multiplicative) model damped
fit4 <- ets(ausair, model = "ZMN", damped = TRUE, alpha = 0.8, beta = 0.2, phi = 0.85)
pred4 <- forecast(fit4, h = 5)

autoplot(ausair) +
  autolayer(pred1$mean, color = "blue") +
  autolayer(pred2$mean, color = "blue", linetype = "dashed") +
  autolayer(pred3$mean, color = "red") +
  autolayer(pred4$mean, color = "red", linetype = "dashed")
```

The above models were for illustrative purposes only.  You would apply the same process as you saw in earlier sections to identify if a damped model predicts more accurately than a non-dampped model.  You can even apply the approaches you saw earlier for tuning this parameter to identify the optimal $\phi$ coefficient.

<br>

## Exercises {#exercises}

Use the `usmelec` data set which is provided by the `fpp2` package.

1. Discuss the merits of using a simple exponential smoothing model versus a Holt's or Holt-Winters model on this data.
2. Partition the data so that you can forecast and assess your predictions for 2012-2013.
3. Identify the exponential smoothing model that minimizes prediction error (RMSE & MAPE) for 2012-2013.
4. Can you tune the smoothing parameters ($\alpha, \beta, \gamma, \phi$) to optimize predictive accuracy?
5. Plot your final model's forecast against the actual data for 2012-2013.
