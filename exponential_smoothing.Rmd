---
title: "R Notebook"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, collapse = TRUE)
```

Exponential forecasting is another smoothing method and has been around since the 1950s.  Where [niave forecasting](ts_benchmarking#naive) places 100% weight on the most recent observation and [moving averages](ts_moving_averages) place equal weight on *k* values, exponential smoothing allows for weighted averages where greater weight can be placed on recent observations and lesser weight on older observations. Exponential smoothing methods are intuitive, computationally efficient, and generally applicable to a wide range of time series. Consequently, exponentially smoothing is a great forecasting tool to have and this tutorial will walk you through the basics.

<br>

## tl;dr

1. [Replication Requirements](#replication): What you'll need to reproduce the analysis in this tutorial.
2. [Simple Exponential Smoothing](#ses): Technique for data with no trend or seasonality.
3. [Holt's Method](#holts): Technique for data with trend but no seasonality.
4. [Holt-Winters Seasonal Method](#hw): Technique for data with trend *and* seasonality.
5. [Model Selection and Error Calculations](#model-selection-and-error-calculations): Discussion of how to calculate errors in exponential forecasting
6. [Prediction Intervals](#prediction-intervals): Using software to simulate prediction intervals for the predicted data points


<br>

## Replication Requirements {#replication}

This tutorial uses the `forecast` and `fpp2` packages.

```{r message=FALSE}
library(tidyverse)
library(forecast)
library(fpp2)          
```

Furthermore, we'll use a couple data sets to illustrate.  The `goog` and `qcement` data are provided by the fpp2 package. Let's go ahead and set up training and validation sets:

```{r}
# create training and validation of the Google stock data
goog.train <- window(goog, end = 900)
goog.test <- window(goog, start = 901)

# create training and validation of the AirPassengers data
qcement.train <- window(qcement, end = c(2012, 4))
qcement.test <- window(qcement, start = c(2013, 1))
```


<br>

## Simple Exponential Smoothing {#ses}

The simplest of the exponentially smoothing methods is called "simple exponential smoothing" (SES).  The key point to remember is that SES is **suitable for data with no trend or seasonal pattern**. This section will illustrate why.

For exponential smoothing, we weigh the recent observations more heavily than older observations. The weight of each observation is determined through the use of a *smoothing parameter*, which we will denote $\alpha$.  For a data set with $T$ observations, we calculate our predicted value, $\hat{y}_{T+1}$, which will be based on $y_{1}$ through $y_{T}$ as follows:

$$
\hat{y}_{T+1} = \alpha{y_T} + \alpha(1-\alpha)y_{T-1} + \dots + \alpha(1-\alpha)^{T-1}y_{1}
$$

where $0 < \alpha \leq 1$. It is also common to come to use the *component form* of this model, which uses the following set of equations.

$$
\hat{y}_{t+1} = l_{t}
$$

$$
l_{t} = \alpha{y_{t}} + (1 - \alpha)l_{t-1}
$$

In both equations we can see that the most weight is placed on the most recent observation. In practice, $\alpha$ equal to 0.1-0.2 tends to perform quite well but we'll demonstrate shortly how to tune this parameter.  When $\alpha$ is closer to 0 we consider this *slow learning* because the algorithm gives historical data more weight.  When $\alpha$ is closer to 1 we consider this *fast learning* because the algorithm gives more weight to the most recent observation; therefore, recent changes in the data will have a bigger impact on forecasted values. The following table illustrates how weighting changes based on the $\alpha$ parameter:

<div id="alpha-chart" class="section level1" style="width: 100%;">
<table style="font-size:13px;">
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
<col width="20%">
<thead>
<tr class="header">
<th align="left">Observation</th>
<th align="left">$\alpha=0.2$</th>
<th align="left">$\alpha=0.4$</th>
<th align="left">$\alpha=0.6$</th>
<th align="left">$\alpha=0.8$</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th align="left">$y_T$</th>
<th align="left">0.2</th>
<th align="left">0.4</th>
<th align="left">0.6</th>
<th align="left">0.8</th>
</tr>
<tr class="even">
<th align="left">$y_{T-1}$</th>
<th align="left">0.16</th>
<th align="left">0.24</th>
<th align="left">0.26</th>
<th align="left">0.16</th>
</tr>
<tr class="odd">
<th align="left">$y_{T-2}$</th>
<th align="left">0.128</th>
<th align="left">0.144</th>
<th align="left">0.096</th>
<th align="left">0.032</th>
</tr>
<tr class="even">
<th align="left">$y_{T-3}$</th>
<th align="left">0.1024</th>
<th align="left">0.0864</th>
<th align="left">0.0384</th>
<th align="left">0.0064</th>
</tr>
<tr class="odd">
<th align="left">$\vdots$</th>
<th align="left">$\vdots$</th>
<th align="left">$\vdots$</th>
<th align="left">$\vdots$</th>
<th align="left">$\vdots$</th>
</tr>
</tbody>
</table>
</div>

Let's go ahead and apply SES to the Google data using the `ses` function.  We manually set the $\alpha = .2$ for our initial model and forecast forward 100 steps with $h=100$. We see that our forecast projects a flatlined estimate into the future, which does not capture the positive trend in the data. This is why SES should not be used on data with a trend or seasonal component.

```{r, fig.align='center', fig.height=4, fig.width=8}
ses.goog <- ses(goog.train, alpha = .2, h = 100)
autoplot(ses.goog)
```

One approach to correct for this is to difference our data to remove the trend.  Now, `goog.dif` represents the change in stock price from the previous day.

```{r, fig.align='center', fig.height=4, fig.width=8}
goog.dif <- diff(goog.train)
autoplot(goog.dif)
```

Once we've differenced we've effectively removed the trend from our data and can reapply the SES model.

```{r, fig.align='center', fig.height=4, fig.width=8}
ses.goog.dif <- ses(goog.dif, alpha = .2, h = 100)
autoplot(ses.goog.dif)
```

To understand how well the model predicts we can compare our forecasts to our validation data set.  But first we need to create a differenced validation set since our training data was built on differenced data.  We see that performance measures are smaller on the test set than the training so we are not overfitting our model.

```{r}
goog.dif.test <- diff(goog.test)
accuracy(ses.goog.dif, goog.dif.test)
```

In our model we used the standard $\alpha = 0.20$; however, we can tune our alpha parameter to identify the value that reduces our forecasting error.  Here we loop through alpha values from 0.01-0.99 and identify the level that minimizes our test RMSE.  Turns out that $\alpha = 0.05$ minimizes our prediction error.

```{r, fig.align='center', fig.height=4, fig.width=6}
# identify optimal alpha parameter
alpha <- seq(.01, .99, by = .01)
RMSE <- NA
for(i in seq_along(alpha)) {
  fit <- ses(goog.dif, alpha = alpha[i], h = 100)
  RMSE[i] <- accuracy(fit, goog.dif.test)[2,2]
}

# convert to a data frame and idenitify min alpha value
alpha.fit <- data_frame(alpha, RMSE)
alpha.min <- filter(alpha.fit, RMSE == min(RMSE))

# plot RMSE vs. alpha
ggplot(alpha.fit, aes(alpha, RMSE)) +
  geom_line() +
  geom_point(data = alpha.min, aes(alpha, RMSE), size = 2, color = "blue")  
```

Now we can re-fit out SES with $\alpha = 0.05$.  Our performance metrics are not significantly different from our model where $\alpha = 0.20$; however, you will notice that the predicted confidence intervals are narrower (left chart).  And when we zoom into the predicted versus actuals (right chart) you see that for most observations, our predicted confidence intervals did well.

```{r, fig.align='center', fig.height=4, fig.width=10}
# refit model with alpha = .05
ses.goog.opt <- ses(goog.dif, alpha = .05, h = 100)

# performance eval
accuracy(ses.goog.opt, goog.dif.test)

# plotting results
p1 <- autoplot(ses.goog.opt) +
  theme(legend.position = "bottom")
p2 <- autoplot(goog.dif.test) +
  autolayer(ses.goog.opt, alpha = .5) +
  ggtitle("Predicted vs. actuals for the test data set")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

<br>

## Holt's Method {#holts}

As mentioned and observed in the previous section, SES does not perform well with data that has a long-term trend. In the last section we illustrated how you can remove the trend with differencing and then perform SES.  An alternative method to apply exponential smoothing while capturing trend in the data is to use _Holt's Method_.  

Holt's Method makes predictions for data with a trend using _**two**_ smoothing parameters, $\alpha$ and $\beta$, which correspond to the level and trend components, respectively. For Holt's method, the prediction will be a line of some non-zero slope that extends from the time step after the last collected data point onwards.

The methodology for predictions using data with a trend (Holt's Method) uses the following equation with $T$ observations. The *k*-step-ahead forecast is given by combining the level estimate at time *t* ($L_t$) and the trend estimate (which in this example is assumbed additive) at time *t* ($T_t$). 

$$
\hat{y}_{T+1} = L_t + kT_t
$$

The level ($L_t$) and trend ($T_t$) are updated through a pair of updating equations, which is where you see the presence of the two smoothing paramters:

$$
L_t = \alpha{y_t} + \alpha(1-\alpha) (L_{t-1} + T_{t-1}),
$$

$$
T_t = \beta(L_t - L_{t-1}) + (1-\beta)T_{t-1}.
$$

In these equations, the first means that the level at time *t* is a weighted average of the actual value at time *t* and the level in the previous period, adjusted for trend.  The second equation means that the trend at time *t* is a weighted average of the trend in the previous period and the more recent information on the change in the level. Similar to SES, $\alpha$ and $\beta$ are constrained to 0-1 with higher values giving faster learning and lower values providing slower learning.

To capture a _**multiplicative**_ (exponential) trend we make a minor adjustment in the above equations:

$$
\hat{y}_{T+1} = L_t \times kT_t
$$

$$
L_t = \alpha{y_t} + \alpha (1 - \alpha) (L_{t-1} \times {T_t-1}),
$$

$$
T_t = \beta(L_t \div L_{t-1}) + (1-\beta)T_{t-1}.
$$

Holt's method also has the alternative _Component Form_ operations.  In this case these represent the additive trend component form:

$$
\hat{y}_{t+h} = l_{t} + hb_{t}
$$

$$
l_{t} = {\alpha}y_{t} + (1 - \alpha)(l_{t-1} + b_{t-1})
$$
$$
b_{t} = {\beta}(l_{t} - l_{t-1}) + (1 - \beta)b_{t-1}
$$

If we go back to our Google stock data, we can apply Holt's method in the following manner.  Here, we will not manually set the $\alpha$ and $\beta$ for our initial model and forecast forward 100 steps with $h=100$. We see that our forecast now does a better job capturing the positive trend in the data.

```{r, fig.align='center', fig.height=4, fig.width=8}
holt.goog <- holt(goog.train, h = 100)
autoplot(holt.goog)
```

Within `holt` you can manually set the $\alpha$ and $\beta$ parameters; however, if you leave those parameters as NULL, the `holt` function will actually identify the optimal model parameters. It does this by minimizing AIC and BIC values.  We can see the model selected by `holt`. In this case, $\alpha = 0.9967$ meaning fast learning in the day-to-day movements and $\beta = 0.0001$ which means slow learning for the trend. 

```{r}
holt.goog$model
```

Let's check the predictive accuracy of our model.  According to our MAPE we have about a 2% error rate.

```{r}
accuracy(holt.goog, goog.test)
```

Similar to SES, we can tune the $\beta$ parameter to see if we can improve our predictive accuracy.  The `holt` function identified an optimal $\beta = 0.0001$; however, this optimal value is based on minimizing errors on the training set, not minimizing prediction errors on the test set.  Let's assess a tradespace of $\beta$ values and see if we gain some predictive accuracy.  Here, we loop through a series of $\beta$ values starting at 0.0001 all the way up to 0.5.  We see that there is a dip in our RMSE at 0.0601.

```{r, fig.align='center', fig.height=4, fig.width=6}
# identify optimal alpha parameter
beta <- seq(.0001, .5, by = .001)
RMSE <- NA
for(i in seq_along(beta)) {
  fit <- holt(goog.train, beta = beta[i], h = 100)
  RMSE[i] <- accuracy(fit, goog.test)[2,2]
}

# convert to a data frame and idenitify min alpha value
beta.fit <- data_frame(beta, RMSE)
beta.min <- filter(beta.fit, RMSE == min(RMSE))

# plot RMSE vs. alpha
ggplot(beta.fit, aes(beta, RMSE)) +
  geom_line() +
  geom_point(data = beta.min, aes(beta, RMSE), size = 2, color = "blue")  
```

Now let's refit our model with this optimal $\beta$ value and compare our predictive accuracy to our original model.  We see that our new model reduces our error rate (MAPE) down to 1.78%.  

```{r}
# new model with optimal beta
holt.goog.opt <- holt(goog.train, h = 100, beta = 0.0601)

# accuracy of first model
accuracy(holt.goog, goog.test)

# accuracy of new optimal model
accuracy(holt.goog.opt, goog.test)
```

If we plot our original versus more recent optimal model we'll notice a couple things.  First, our predicted values for the optimal model are more conservative; in other words they are assuming a more gradual slope. Second, the confidence intervals are much more extreme.  So although our predictions were more accurate, our uncertainty increases.  The reason for this is that by increasing our $\beta$ value we are assuming faster learning from more recent observations.  And since there some quite a bit of turbulence in the recent time period, this is causing greater variance to be incorporated into our prediction intervals.  This requires a more indepth discussion than this tutorial will go into, but the important thing to keep in mind is that although we increase our prediction accuracy with parameter tuning, there are additional side effects that can occur, which may be harder to explain to decision-makers.

```{r, fig.align='center', fig.height=4, fig.width=10}
p1 <- autoplot(holt.goog) +
  ggtitle("Original Holt's Model") +
  coord_cartesian(ylim = c(400, 1000))

p2 <- autoplot(holt.goog.opt) +
  ggtitle("Optimal Holt's Model") +
  coord_cartesian(ylim = c(400, 1000))

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

<br>

## Holt-Winters Seasonal Method {#hw}

To make predictions using data with a trend and seasonality, we turn to the Holt-Winters Seasonal Method. This method can be implemented with an "Additive" structure or a "Multiplicative" structure, where the choice of method depends on the data set. The Additive model is best used when the seasonal trend is of the same magnitude throughout the data set, while the Multiplicative Model is preferred when the magnitude of seasonality changes as time increases.

Since the Google data does not have seasonality, we'll use the `qcement` data that we set up in the [Replication section](#replication).  This data has seasonality and trend; however, it is unclear if seasonality is additive or multiplicative.  We'll use the Holt-Winters method to identify the best fit model.

### Additive
For the Additive model, the regular equation form is:


where $\alpha$, $\beta$, and $\gamma$ are the three smoothing parameters to deal with the level pattern, the trend, and the seasonality, respectively. Similar to SES and Holt's method, all three parameters are constrained to 0-1.  The component equations are as follows:

$$
\hat{y}_{t+h} = l_{t} + hb_{t} + s_{t-m+h^{+}_{m}}
$$

$$
l_{t} = \alpha(y_{t} - s_{t-m}) + (1 - \alpha)(l_{t-1} + b_{t-1})
$$

$$
b_{t} = \beta(l_{t} - l_{t-1}) + (1 - \beta)b_{t-1}
$$

$$
s_{t} = \gamma(y_{t} - l_{t-1} - b_{t-1}) + (1-\gamma)s_{t-m}
$$




